training_args:
  output_dir: outputs
  num_epochs: 100

  learning_rate: 1e-4
  lr_scheduler_type: constant_with_warmup
  lr_warmup_steps: 500

  train_batch_size: 256
  eval_batch_size: 64
  data_loader_num_workers: 8

  gradient_accumulation_steps: 1
  logger: wandb
  mixed_precision: fp16
  checkpointing_steps: 1000
  resume_from_checkpoint: null

  tracker_init_kwargs:
    group: 'ddpm'

training_wrapper:
  _target_: mugen.trainers.ddpm_trainer.DDPMTrainingWrapper
  # pretrained_name_or_path: google/ddpm-cifar10-32
  unet_config:
    sample_size: 32
    block_out_channels: [128, 256, 256, 256]
    layers_per_block: 2
    down_block_types: ['DownBlock2D', 'AttnDownBlock2D', 'DownBlock2D', 'DownBlock2D']
    up_block_types: ['UpBlock2D', 'UpBlock2D', 'AttnUpBlock2D', 'UpBlock2D']
  scheduler_config:
    num_train_timesteps: 1000
    beta_schedule: 'linear'
    beta_start: 1e-4
    beta_end: 2e-2
  use_ema: true
  enable_xformers_memory_efficient_attention: true

datamodule:
  _target_: mugen.datamodules.ImageDataModule
  dataset_name: cifar10
  train_split: train
  val_split: test
  image_column: img
  resolution: 32
