training_args:
  output_dir: outputs
  num_epochs: 200

  learning_rate: 5e-6
  lr_scheduler_type: constant_with_warmup
  warmup_steps: 1000

  train_batch_size: 16
  eval_batch_size: 2
  data_loader_num_workers: 8

  eval_steps: 2000

  gradient_accumulation_steps: 1
  mixed_precision: fp16
  use_8bit_adam: true
  use_lora: true
  lora_rank: 8
  lora_alpha: 32
  use_effective_conv2d: true

  logger: wandb
  save_steps: 1000
  save_total_limit: 3
  resume_from_checkpoint: latest
  tracker_init_kwargs:
    group: 'anime-face'
    name: 'sd-32-anime-face'
    tags: ['anime-face', 'sd-32']

training_module:
  _target_: mugen.trainingmodules.text2image.Text2ImageTrainingModule
  pretrained_name_or_path: Ojimi/anime-kawai-diffusion
  tokenizer_pretrained_name_or_path: outputs/clip_anime_face/checkpoint-4000/tokenizer
  text_encoder_pretrained_name_or_path: outputs/clip_anime_face/checkpoint-4000/text_encoder

  use_ema: false
  enable_xformers_memory_efficient_attention: true
  enable_gradient_checkpointing: true
  run_safety_checker: false

datamodule:
  _target_: mugen.datamodules.Text2ImageDataModule
  data_path: huanngzh/anime_face_control_60k
  train_split: train[:95%]
  val_split: train[95%:]
  image_column: target
  caption_column: prompt
  resolution: 512
  random_flip: false
  pipeline_name_or_path: ${training_module.pretrained_name_or_path}
  batch_size: 4
#  load_cached: false
